# 循环神经网络与`word2vec`

循环神经网络是一类非常特殊的神经网络，通常的神经网络并不会考虑数据与数据之间的耦合联系而是把它们当成独立同分布的样本。但事实是很多数据都是存在相互耦合的。比如在NLP（自然语言处理）上，一句话的词语和词语之间是存在很强的时间上的耦合关系的。为了处理这种耦合关系循环神经网络在原有神经网络的基础上进行了改进，让神经元的状态能够在时间序列上传递。当然，这样的传递是存在衰减的，通常并不能传递太远的距离，这也是传统的循环神经网络的一大问题。

![](https://github.com/yhswjtuILMARE/Machine-Learning-Study-Notes/blob/master/pics/rnn.png)

上图是最基础的循环神经网络的神经元示例，其中仅仅包含一个双曲正切作为激活函数。如果想要了解更多请移步我的博客[ILMARE的博客](https://www.ilmareblog.com)，参看其中的文章`循环神经网络（RNN）浅析`。

`word2vec`是另一类自然语言处理问题。通常的有监督机器学习问题都会将数据转化成多维向量和标记的形式，这些多维向量用于表述数据的种种特征，这样的方式在自然语言处理上遇到了问题，因为组成自然语言的最小单位——词组很难用特征去表示，如果使用`one-hot`编码来向量化词组就会不可避免地造成向量过于庞大，而且完全无法体现出自然语言单词与单词之间的关系。基于以上的考虑谷歌公司提出了`word2vec`技术，这是一种基于概率学的词组编码技术，非常复杂。具体的推导以及证明请移步我的博客[ILMARE的博客](https://www.ilmareblog.com)，请参阅文章`对Word2Vec的一点理解`。

这个仓库主要包含我对循环神经网络和`word2vec`的一些实践：我基于RNN制作了一个可以自动生成文本的程序，这个Demo的灵感来源于另一个开源项目[Char-RNN-TensorFlow](https://github.com/hzy46/Char-RNN-TensorFlow)；还有就是实现了`word2vec`；

## 循环神经网络

这个小Demo是用于生成文本的，通俗地说就是让AI来写文章。这个Demo的基本思想是这样的：首先将语料库中的词或字进行提取，对其中最频繁出现的前N个进行编号。在进行训练时我们以句子作为训练的基本单位，首先假设M个单词或字构成一个句子然后将这些词按照其在句子中出现的先后顺序随机编码成向量（之所以是随机的是因为在这里我们仅仅是想学习词或字出现的先后规律而并不关心这个词或字本身的意义）后输入到神经网络中进行训练，训练的目标或是预测的label就是这个句子出现的下一个字。

模型训练好之后仅仅需要提供一个字，词或者句子作为种子（甚至不需要提供）就可以让网络生成文本，这样做的原理就是基于我们输入的文本预测下一个即将出现的文本，然后再根据预测出的文本进行下一次预测，直到某一个终止条件（例如我们给出的最大生成字或词的数目）停止。

在这里，我使用刘慈欣的三体小说作为语料库，进行了一些迭代。由于时间关系并没有训练太久，但是可以观察到模型的的确确是在收敛中，最终在训练了5000个epoch后终止。我们使用`程心`作为种子看看那能生成什么样的文字：

```
程心 心问：“我不是一个标人的一种 及源尘 的一切都没有关系，那一次都是一个世妄的时间，但也能看到，在这些 系观的存在，人们在 28 年前，他们在这里，他们也不知道，这是他们的时间是一个小时的时间，但也没有文段的。”

　　“我的！” 程心想起了一种声冷，“我的！” AA 的话学她说，“我的我也是 一切都有人的东西。这时，他们也没见过来。”关一帆走过程心的话。他说，“那是在我的人都不会，我们不知道那话的，他们不是这十的话。 他们在这个范像是 一种死票的 辨V，在她的 几睛， 她们也不是为了一个标座的。

　　在他的手指上了， “我想起来，她的几睛就是这十。”

　　“我不是我们的时间里，她在那个世界是 一个世界，她的几睛，她就在这个时代的一个人就是 一个世 界的世界。 他的目光中是一个人， 她不知道，我们在这种 话的话，他们的目光，但她的一切都不是 一个人。”

　　“我不知道，我们的生活，那个时代，他也不是一个人的那个世界的，她不是一个人，不管她是我。” 程心灯灯头。” 程心灯灯头，“你的目光中，那是一个 一个 方面的 草穷来时 代，在那一次，我的人都没有看，他们 都不再 是一个人的那种，
```
可以看到：程序生成的文本还是十分生硬和不通顺，这可能是由于训练不足导致的。不过有意思的是：以“程心”作为种子生成的文本中几乎都出现了“关一帆”的名字，这也说明了在小说中程心和关一帆的联系真的很紧密，以至于网络都将他们视为了拥有很强耦合关系的部分。

## `word2vec`

这一部分有很强的数学基础，很不容易进行表述，如果想要了解更多请访问我的博客，地址在上面给出了。不过本例实现的`word2vec`原理用最简明的语句进行表述就是：假如有一个句子`I want to play basketball`，我们对其中的单词进行编码时要考虑它和上下文的关系，比如`<want, to>`，`<want, I>`都是对`want`进行编码时需要考虑的。在这样的设定下，如果有另一个语义用法和`want`相近的词，那么其上下文也必然和`want`相似，这样那个词获得的编码向量和`want`的向量就会很接近。

以下是实验得出的结果，我随机选取了16个单词并打印出了在编码上和它们最接近的8个词，`<>`中即是词向量的欧氏距离：

```
Nearst to "three" : four - <0.740>, five - <0.706>, two - <0.706>, zero - <0.703>, six - <0.695>, eight - <0.691>, seven - <0.661>, one - <0.633>
Nearst to "would" : can - <0.426>, to - <0.424>, transformation - <0.421>, says - <0.421>, adults - <0.416>, had - <0.398>, instrumental - <0.396>, like - <0.388>
Nearst to "known" : repeatedly - <0.413>, regarded - <0.394>, falls - <0.394>, joint - <0.375>, available - <0.369>, sir - <0.365>, UNK - <0.364>, select - <0.362>
Nearst to "world" : holding - <0.423>, died - <0.415>, holy - <0.394>, first - <0.393>, wayne - <0.391>, sea - <0.390>, mathbf - <0.382>, focus - <0.379>
Nearst to "history" : UNK - <0.424>, identical - <0.419>, state - <0.418>, orbital - <0.415>, next - <0.413>, boston - <0.411>, list - <0.410>, frequent - <0.408>
Nearst to "but" : however - <0.503>, that - <0.462>, ties - <0.462>, sky - <0.456>, exposed - <0.452>, generally - <0.441>, because - <0.430>, landscape - <0.414>
Nearst to "most" : supporters - <0.419>, earned - <0.406>, friendly - <0.405>, anarchist - <0.397>, absence - <0.394>, articles - <0.389>, apparent - <0.386>, difference - <0.385>
Nearst to "states" : co - <0.477>, symbolic - <0.428>, market - <0.412>, by - <0.408>, skills - <0.396>, legs - <0.395>, in - <0.392>, alphabet - <0.382>
Nearst to "only" : respectively - <0.434>, powerful - <0.404>, warming - <0.400>, transform - <0.392>, showing - <0.391>, theory - <0.389>, georgia - <0.383>, adventures - <0.382>
Nearst to "while" : and - <0.411>, enlightenment - <0.409>, started - <0.405>, conduct - <0.403>, convention - <0.396>, academy - <0.390>, lived - <0.390>, novels - <0.384>
Nearst to "other" : additional - <0.418>, fall - <0.415>, recovery - <0.398>, caught - <0.391>, gene - <0.390>, egyptian - <0.386>, different - <0.382>, some - <0.371>
Nearst to "which" : that - <0.490>, this - <0.435>, max - <0.424>, and - <0.410>, images - <0.408>, success - <0.407>, benefit - <0.404>, five - <0.403>
Nearst to "they" : why - <0.401>, assassination - <0.401>, he - <0.400>, occasions - <0.400>, southwest - <0.391>, inherited - <0.383>, democrats - <0.382>, norwegian - <0.378>
Nearst to "as" : detail - <0.459>, constitutional - <0.432>, error - <0.416>, cd - <0.412>, compiler - <0.407>, million - <0.405>, certain - <0.404>, creation - <0.397>
Nearst to "use" : deal - <0.438>, maintain - <0.406>, sphere - <0.392>, office - <0.390>, energy - <0.388>, effects - <0.385>, anarchist - <0.384>, every - <0.382>
Nearst to "has" : had - <0.468>, have - <0.455>, is - <0.439>, representative - <0.415>, was - <0.398>, kind - <0.395>, jordan - <0.385>, speech - <0.378>
```
结果中`<>`内部的数字是相似程度。观察结果的第一条：

```
Nearst to "three" : four - <0.740>, five - <0.706>, two - <0.706>, zero - <0.703>, six - <0.695>, eight - <0.691>, seven - <0.661>, one - <0.633>
```
可以看出，和“three”语义相近的词汇均是英文中的数字词汇，且相似度都在60%以上说明这个模型的效果还是比较不错的。